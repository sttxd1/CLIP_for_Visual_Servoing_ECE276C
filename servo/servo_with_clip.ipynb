{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import cv2\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import pinv\n",
    "from scipy.spatial.transform import Rotation as Rot\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# CLIP SETUP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def encode_text(text):\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features\n",
    "\n",
    "def encode_image(pil_img):\n",
    "    image_input = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera (don't change these settings)\n",
    "camera_width = 512                                             #image width\n",
    "camera_height = 512                                            #image height\n",
    "camera_fov = 120                                                #field of view of camera\n",
    "camera_focal_depth = 0.5*camera_height/np.tan(0.5*np.pi/180*camera_fov) \n",
    "                                                               #focal depth in pixel space\n",
    "camera_aspect = camera_width/camera_height                     #aspect ratio\n",
    "camera_near = 0.02                                             #near clipping plane in meters, do not set non-zero\n",
    "camera_far = 100                                               #far clipping plane in meters\n",
    "\n",
    "\n",
    "#control objectives (if you wish, you can play with these values for fun)\n",
    "object_location_desired = np.array([camera_width/2,camera_height/2])\n",
    "                                                               #center the object to middle of image \n",
    "K_p_x = 0.1                                                    #Proportional control gain for translation\n",
    "K_p_Omega = 0.02                                               #Proportional control gain for rotation  \n",
    "\n",
    "# Robot with Camera Class\n",
    "class eye_in_hand_robot:\n",
    "    def get_ee_position(self):\n",
    "        '''\n",
    "        Function to return the end-effector of the link. This is the very tip of the robot at the end of the jaws.\n",
    "        '''\n",
    "        endEffectorIndex = self.numActiveJoints\n",
    "        endEffectorState = p.getLinkState(self.robot_id, endEffectorIndex)\n",
    "        endEffectorPos = np.array(endEffectorState[0])\n",
    "        endEffectorOrn = np.array(p.getMatrixFromQuaternion(endEffectorState[1])).reshape(3,3)\n",
    "        \n",
    "        #add an offset to get past the forceps\n",
    "        endEffectorPos += self.camera_offset*endEffectorOrn[:,2]\n",
    "        return endEffectorPos, endEffectorOrn\n",
    "\n",
    "    def get_current_joint_angles(self):\n",
    "        # Get the current joint angles\n",
    "        joint_angles = np.zeros(self.numActiveJoints)\n",
    "        for i in range(self.numActiveJoints):\n",
    "            joint_state = p.getJointState(self.robot_id, self._active_joint_indices[i])\n",
    "            joint_angles[i] = joint_state[0]\n",
    "        return joint_angles\n",
    "    \n",
    "    def get_jacobian_at_current_position(self):\n",
    "        #Returns the Robot Jacobian of the last active link\n",
    "        mpos, mvel, mtorq = self.get_active_joint_states()   \n",
    "        zero_vec = [0.0]*len(mpos)\n",
    "        linearJacobian, angularJacobian = p.calculateJacobian(self.robot_id, \n",
    "                                                              self.numActiveJoints,\n",
    "                                                              [0,0,self.camera_offset],\n",
    "                                                              mpos, \n",
    "                                                              zero_vec,\n",
    "                                                              zero_vec)\n",
    "        #only return the active joint's jacobians\n",
    "        Jacobian = np.vstack((linearJacobian,angularJacobian))\n",
    "        return Jacobian[:,:self.numActiveJoints]\n",
    "    \n",
    "    def set_joint_position(self, desireJointPositions, kp=1.0, kv=0.3):\n",
    "        '''Set  the joint angle positions of the robot'''\n",
    "        zero_vec = [0.0] * self._numLinkJoints\n",
    "        allJointPositionObjectives = [0.0]*self._numLinkJoints\n",
    "        for i in range(desireJointPositions.shape[0]):\n",
    "            idx = self._active_joint_indices[i]\n",
    "            allJointPositionObjectives[idx] = desireJointPositions[i]\n",
    "\n",
    "        p.setJointMotorControlArray(self.robot_id,\n",
    "                                    range(self._numLinkJoints),\n",
    "                                    p.POSITION_CONTROL,\n",
    "                                    targetPositions=allJointPositionObjectives,\n",
    "                                    targetVelocities=zero_vec,\n",
    "                                    positionGains=[kp] * self._numLinkJoints,\n",
    "                                    velocityGains=[kv] * self._numLinkJoints)\n",
    "\n",
    "    def get_active_joint_states(self):\n",
    "        '''Get the states (position, velocity, and torques) of the active joints of the robot\n",
    "        '''\n",
    "        joint_states = p.getJointStates(self.robot_id, range(self._numLinkJoints))\n",
    "        joint_infos = [p.getJointInfo(self.robot_id, i) for i in range(self._numLinkJoints)]\n",
    "        joint_states = [j for j, i in zip(joint_states, joint_infos) if i[3] > -1]\n",
    "        joint_positions = [state[0] for state in joint_states]\n",
    "        joint_velocities = [state[1] for state in joint_states]\n",
    "        joint_torques = [state[3] for state in joint_states]\n",
    "        return joint_positions, joint_velocities, joint_torques\n",
    "\n",
    "\n",
    "         \n",
    "    def __init__(self, robot_id, initialJointPos):\n",
    "        self.robot_id = robot_id\n",
    "        self.eeFrameId = []\n",
    "        self.camera_offset = 0.1 #offset camera in z direction to avoid grippers\n",
    "        # Get the joint info\n",
    "        self._numLinkJoints = p.getNumJoints(self.robot_id) #includes passive joint\n",
    "        jointInfo = [p.getJointInfo(self.robot_id, i) for i in range(self._numLinkJoints)]\n",
    "        \n",
    "        # Get joint locations (some joints are passive)\n",
    "        self._active_joint_indices = []\n",
    "        for i in range(self._numLinkJoints):\n",
    "            if jointInfo[i][2]==p.JOINT_REVOLUTE:\n",
    "                self._active_joint_indices.append(jointInfo[i][0])\n",
    "        self.numActiveJoints = len(self._active_joint_indices) #exact number of active joints\n",
    "\n",
    "        #reset joints\n",
    "        for i in range(self._numLinkJoints):\n",
    "            p.resetJointState(self.robot_id,i,initialJointPos[i])\n",
    "\n",
    "\n",
    "def draw_coordinate_frame(position, orientation, length, frameId = []):\n",
    "    '''\n",
    "    Draws a coordinate frame x,y,z with scaled lengths on the axes \n",
    "    in a position and orientation relative to the world coordinate frame\n",
    "    pos: 3-element numpy array\n",
    "    orientation: 3x3 numpy matrix\n",
    "    length: length of the plotted x,y,z axes\n",
    "    frameId: a unique ID for the frame. If this supplied, then it will erase the previous location of the frame\n",
    "    \n",
    "    returns the frameId\n",
    "    '''\n",
    "    if len(frameId)!=0:\n",
    "        p.removeUserDebugItem(frameId[0])\n",
    "        p.removeUserDebugItem(frameId[1])\n",
    "        p.removeUserDebugItem(frameId[2])\n",
    "    \n",
    "    lineIdx=p.addUserDebugLine(position, position + np.dot(orientation, [length, 0, 0]), [1, 0, 0])  # x-axis in red\n",
    "    lineIdy=p.addUserDebugLine(position, position + np.dot(orientation, [0, length, 0]), [0, 1, 0])  # y-axis in green\n",
    "    lineIdz=p.addUserDebugLine(position, position + np.dot(orientation, [0, 0, length]), [0, 0, 1])  # z-axis in blue\n",
    "\n",
    "    return lineIdx,lineIdy,lineIdz\n",
    "\n",
    "def opengl_plot_world_to_pixelspace(pt_in_3D_to_project, viewMat, projMat, imgWidth, imgHeight):\n",
    "    ''' Plots a x,y,z location in the world in an openCV image\n",
    "    This is used for debugging, e.g. given a known location in the world, verify it appears in the camera\n",
    "    when using p.getCameraImage(...). The output [u,v], when plot with opencv, should line up with object \n",
    "    in the image from p.getCameraImage(...)\n",
    "    '''\n",
    "    pt_in_3D_to_project = np.append(pt_in_3D_to_project,1)\n",
    "    #print('Point in 3D to project:', pt_in_3D_to_project)\n",
    "\n",
    "    pt_in_3D_in_camera_frame = viewMat @ pt_in_3D_to_project\n",
    "    #print('Point in camera space: ', pt_in_3D_in_camera_frame)\n",
    "\n",
    "    # Convert coordinates to get normalized device coordinates (before rescale)\n",
    "    uvzw = projMat @ pt_in_3D_in_camera_frame\n",
    "    #print('after projection: ', uvzw)\n",
    "\n",
    "    # scale to get the normalized device coordinates\n",
    "    uvzw_NDC = uvzw/uvzw[3]\n",
    "    #print('after normalization: ', uvzw_NDC)\n",
    "\n",
    "    #x,y specifies lower left corner of viewport rectangle, in pixels. initial value is (0,)\n",
    "    u = ((uvzw_NDC[0] + 1) / 2.0) * imgWidth\n",
    "    v = ((1-uvzw_NDC[1]) / 2.0) * imgHeight\n",
    "\n",
    "    return [int(u),int(v)]\n",
    "\n",
    "    \n",
    "def get_camera_view_and_projection_opencv(cameraPos, camereaOrn):\n",
    "    '''Gets the view and projection matrix for a camera at position (3) and orientation (3x3)'''\n",
    "    __camera_view_matrix_opengl = p.computeViewMatrix(cameraEyePosition=cameraPos,\n",
    "                                                   cameraTargetPosition=cameraPos+camereaOrn[:,2],\n",
    "                                                   cameraUpVector=-camereaOrn[:,1])\n",
    "\n",
    "    __camera_projection_matrix_opengl = p.computeProjectionMatrixFOV(camera_fov, camera_aspect, camera_near, camera_far)        \n",
    "    _, _, rgbImg, depthImg, _ = p.getCameraImage(camera_width, \n",
    "                                                 camera_height, \n",
    "                                                 __camera_view_matrix_opengl,\n",
    "                                                 __camera_projection_matrix_opengl, \n",
    "                                                 renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "    #returns camera view and projection matrices in a form that fits openCV\n",
    "    viewMat = np.array(__camera_view_matrix_opengl).reshape(4,4).T\n",
    "    projMat = np.array(__camera_projection_matrix_opengl).reshape(4,4).T\n",
    "    return viewMat, projMat\n",
    "\n",
    "def get_camera_img_float(cameraPos, camereaOrn):\n",
    "    ''' Gets the image and depth map from a camera at a position cameraPos (3) and cameraOrn (3x3) in space. '''\n",
    "    __camera_view_matrix_opengl = p.computeViewMatrix(cameraEyePosition=cameraPos,\n",
    "                                                   cameraTargetPosition=cameraPos+camereaOrn[:,2],\n",
    "                                                   cameraUpVector=-camereaOrn[:,1])\n",
    "\n",
    "    __camera_projection_matrix_opengl = p.computeProjectionMatrixFOV(camera_fov, camera_aspect, camera_near, camera_far)        \n",
    "    width, height, rgbImg, nonlinDepthImg, _ = p.getCameraImage(camera_width, \n",
    "                                                 camera_height, \n",
    "                                                 __camera_view_matrix_opengl,\n",
    "                                                 __camera_projection_matrix_opengl, \n",
    "                                                 renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "    #adjust for clipping and nonlinear distance i.e., 1/d (0 is closest, i.e., near, 1 is furthest away, i.e., far\n",
    "    depthImgLinearized =camera_far*camera_near/(camera_far+camera_near-(camera_far-camera_near)*nonlinDepthImg)\n",
    "\n",
    "    #convert to numpy and a rgb-d image\n",
    "    rgb_image = np.array(rgbImg[:,:,:3], dtype=np.uint8)\n",
    "    depth_image = np.array(depthImgLinearized, dtype=np.float32)\n",
    "    return rgb_image, depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageJacobian(u_px, v_px, depthImg, focal_length, imgWidth, imgHeight):\n",
    "    \"\"\"Calculate the image Jacobian (interaction matrix) for visual servoing.\"\"\"\n",
    "\n",
    "    f = focal_length\n",
    "    z = depthImg[u_px,v_px]\n",
    "\n",
    "    u = u_px - imgWidth/2\n",
    "    v = v_px - imgHeight/2\n",
    "\n",
    "    L = np.array([\n",
    "        [-f/z, 0 ,-u/z, u*v/f, f+u**2/f,-v],\n",
    "        [0,f/z, -v/z, f+v**2/f, u*v/f, u]\n",
    "    ])\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCameraControl(object_loc_des, object_loc, image_jacobian):\n",
    "    \"\"\"Calculate camera velocity screw with different gains for translation and rotation\"\"\"\n",
    "    error = object_loc_des - np.array(object_loc)\n",
    "    \n",
    "    J_pinv = pinv(image_jacobian)\n",
    "    raw_velocity = np.dot(J_pinv, error)\n",
    "    \n",
    "    # Apply different gains for translation and rotation\n",
    "    velocity_screw = np.zeros(6)\n",
    "    velocity_screw[:3] = 0.1 * raw_velocity[:3]     # Kp = 0.1 for translation\n",
    "    velocity_screw[3:] = 0.02 * raw_velocity[3:]    # Kω = 0.02 for rotation\n",
    "    \n",
    "    delta_X = velocity_screw[:3]\n",
    "    delta_Omega = velocity_screw[3:]\n",
    "    \n",
    "    return delta_X, delta_Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findJointControl(robot, delta_X, delta_Omega):\n",
    "    \"\"\"\n",
    "    Calculate joint velocities using the robot Jacobian and null space optimization\n",
    "    \n",
    "    Parameters:\n",
    "        robot: robot instance containing Jacobian information\n",
    "        delta_X: desired camera linear velocity in world frame\n",
    "        delta_Omega: desired camera angular velocity in world frame\n",
    "    \"\"\"\n",
    "    \n",
    "    J_robot = robot.get_jacobian_at_current_position()\n",
    "    \n",
    "    V_desired = np.concatenate([delta_X, delta_Omega])\n",
    "    \n",
    "    damping = 0.01\n",
    "    J_dag = J_robot.T @ np.linalg.inv(J_robot @ J_robot.T + damping**2 * np.eye(6))\n",
    "    \n",
    "    null_proj = np.eye(robot.numActiveJoints) - J_dag @ J_robot\n",
    "    \n",
    "    q_current = robot.get_current_joint_angles()\n",
    "    \n",
    "    q_comfortable = np.array([0, -np.pi/4, 0, -np.pi/2, 0, np.pi/2, np.pi/4])\n",
    "    \n",
    "    k0 = 0.1  \n",
    "\n",
    "    delta_q = J_dag @ V_desired + k0 * null_proj @ (q_comfortable - q_current)\n",
    "    \n",
    "    return delta_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_boxes(img, num_boxes_horizontal=8, num_boxes_vertical=8):\n",
    "    \"\"\"\n",
    "    Generate candidate bounding boxes by dividing the image into a grid.\n",
    "    Each cell in the grid is considered a candidate bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        img (np.array): The image as a numpy array (H, W, 3).\n",
    "        num_boxes_horizontal (int): Number of candidate boxes along the width.\n",
    "        num_boxes_vertical (int): Number of candidate boxes along the height.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple]: A list of candidate boxes in the form (x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    h, w, _ = img.shape\n",
    "    box_width = w // num_boxes_horizontal\n",
    "    box_height = h // num_boxes_vertical\n",
    "\n",
    "    boxes = []\n",
    "    for i in range(num_boxes_vertical):\n",
    "        for j in range(num_boxes_horizontal):\n",
    "            x1 = j * box_width\n",
    "            y1 = i * box_height\n",
    "            x2 = x1 + box_width\n",
    "            y2 = y1 + box_height\n",
    "            boxes.append((x1, y1, x2, y2))\n",
    "    return boxes\n",
    "\n",
    "def get_best_box(rgb, candidate_boxes, text_features, model, preprocess, device):\n",
    "    # Extract all candidate crops\n",
    "    crops = []\n",
    "    for box in candidate_boxes:\n",
    "        (x1, y1, x2, y2) = box\n",
    "        pil_crop = Image.fromarray(rgb[y1:y2, x1:x2])\n",
    "        crops.append(preprocess(pil_crop).unsqueeze(0))\n",
    "    \n",
    "    # Stack all crops into a single batch tensor\n",
    "    batch = torch.cat(crops, dim=0).to(device)\n",
    "\n",
    "    # Run the model once on the whole batch\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(batch)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute similarities\n",
    "    similarities = (image_features @ text_features.T).squeeze()\n",
    "\n",
    "    # Find best matching box\n",
    "    best_idx = similarities.argmax().item()\n",
    "    best_box = candidate_boxes[best_idx]\n",
    "    best_similarity = similarities[best_idx].item()\n",
    "    return best_box, best_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_final_results(similarity):\n",
    "    \"\"\"Create final summary plots of roll angle and tracking error\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Roll angle subplot\n",
    "    \n",
    "    plt.plot(similarity, label='Similarity Change')\n",
    "    plt.title('Similarity Change')\n",
    "    plt.ylabel('Similarity')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "Initial EE pose: [0.05695421 0.05842742 1.11240143] [[ 0.7072639  -0.01201191  0.70684757]\n",
      " [-0.70694639 -0.00899466  0.70720993]\n",
      " [-0.00213709 -0.9998874  -0.01485337]]\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3070 Laptop GPU/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 535.183.01\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 535.183.01\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3070 Laptop GPU/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n",
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "Thread TERMINATED\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "# =====================\n",
    "# PyBullet Setup\n",
    "p.connect(p.GUI)\n",
    "time_step = 0.001\n",
    "p.resetSimulation()\n",
    "p.setTimeStep(time_step)\n",
    "p.setGravity(0, 0, -9.8)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "p.loadURDF('plane.urdf')\n",
    "\n",
    "\n",
    "\n",
    "# Load Panda\n",
    "pandaUid = p.loadURDF(\"franka_panda/panda.urdf\", useFixedBase=True)\n",
    "initialJointPosition = [0, -np.pi/4, np.pi/4, -np.pi/4, np.pi/4, np.pi/4, np.pi/4, 0, 0, 0, 0, 0]\n",
    "robot = eye_in_hand_robot(pandaUid, initialJointPosition)\n",
    "# p.stepSimulation()\n",
    "\n",
    "angle = np.radians(45)\n",
    "forward = np.array([np.cos(angle), np.sin(angle), 0])\n",
    "up = np.array([0,0,-1])\n",
    "right = np.cross(up, forward)\n",
    "cameraOrientation = np.column_stack((right, up, forward))\n",
    "cameraPosition = np.array([0,0,1])\n",
    "# Convert rotation matrix to quaternion using SciPy\n",
    "rot = R.from_matrix(cameraOrientation)\n",
    "target_quat = rot.as_quat()  # [x, y, z, w]\n",
    "\n",
    "endEffectorIndex = robot.numActiveJoints\n",
    "ik_solution = p.calculateInverseKinematics(pandaUid, endEffectorIndex, cameraPosition, target_quat)\n",
    "\n",
    "# Set joints to IK solution\n",
    "for i, joint_index in enumerate(robot._active_joint_indices):\n",
    "    p.resetJointState(pandaUid, joint_index, ik_solution[i])\n",
    "\n",
    "p.stepSimulation()\n",
    "\n",
    "# Now camera is at the EE with desired orientation/position\n",
    "cameraPosition, cameraOrientation = robot.get_ee_position()\n",
    "print(\"Initial EE pose:\", cameraPosition, cameraOrientation)\n",
    "\n",
    "# Place a box\n",
    "box_length, box_width, box_depth = 0.4, 0.4, 0.4\n",
    "target_box_center = [1.5, 0.5, 0.3]\n",
    "geomBox = p.createCollisionShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2])\n",
    "visualBox = p.createVisualShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2],\n",
    "                                rgbaColor=[1,0,0,1])\n",
    "boxId = p.createMultiBody(baseMass=0,\n",
    "                          baseCollisionShapeIndex=geomBox,\n",
    "                          baseVisualShapeIndex=visualBox,\n",
    "                          basePosition=target_box_center,\n",
    "                          baseOrientation=p.getQuaternionFromEuler([0,0,0]))\n",
    "\n",
    "box2_length, box2_width, box2_depth = 0.2, 0.2, 0.6\n",
    "target_box2_center = [1, 1, 0.6]\n",
    "geomBox2 = p.createCollisionShape(p.GEOM_BOX, halfExtents=[box2_length/2, box2_width/2, box2_depth/2])\n",
    "visualBox2 = p.createVisualShape(p.GEOM_BOX, halfExtents=[box2_length/2, box2_width/2, box2_depth/2],\n",
    "                                rgbaColor=[1,0.5,0,1])\n",
    "box2Id = p.createMultiBody(baseMass=0,\n",
    "                          baseCollisionShapeIndex=geomBox2,\n",
    "                          baseVisualShapeIndex=visualBox2,\n",
    "                          basePosition=target_box2_center,\n",
    "                          baseOrientation=p.getQuaternionFromEuler([0,0,0.5]))\n",
    "\n",
    "# Place a blue sphere\n",
    "sphere_radius = 0.25\n",
    "sphere_position = [0.5, 1.5, 0.35]\n",
    "geomSphere = p.createCollisionShape(p.GEOM_SPHERE, radius=sphere_radius)\n",
    "visualSphere = p.createVisualShape(p.GEOM_SPHERE, radius=sphere_radius, rgbaColor=[0,0,1,1])\n",
    "sphereId = p.createMultiBody(baseMass=0,\n",
    "                             baseCollisionShapeIndex=geomSphere,\n",
    "                             baseVisualShapeIndex=visualSphere,\n",
    "                             basePosition=sphere_position,\n",
    "                             baseOrientation=p.getQuaternionFromEuler([0,0,0]))\n",
    "\n",
    "# Place a green square plate\n",
    "plate_size = 2\n",
    "plate_thickness = 0.1\n",
    "plate_position = [1,1,plate_thickness/2]\n",
    "geomPlate = p.createCollisionShape(p.GEOM_BOX, halfExtents=[plate_size/2, plate_size/2, plate_thickness/2])\n",
    "visualPlate = p.createVisualShape(p.GEOM_BOX, halfExtents=[plate_size/2, plate_size/2, plate_thickness/2],\n",
    "                                  rgbaColor=[0,1,0,1])\n",
    "plateId = p.createMultiBody(baseMass=0,\n",
    "                            baseCollisionShapeIndex=geomPlate,\n",
    "                            baseVisualShapeIndex=visualPlate,\n",
    "                            basePosition=plate_position,\n",
    "                            baseOrientation=p.getQuaternionFromEuler([0,0,0]))\n",
    "\n",
    "\n",
    "# Text description of the object to find\n",
    "target_description = \"The orange rectangular prism on the green plane\"\n",
    "text_features = encode_text(target_description)\n",
    "\n",
    "joint_limits_lower = [-2.8973, -1.7628, -2.8973, -3.0718, -2.8973, -0.0175, -2.8973]\n",
    "joint_limits_upper = [2.8973, 1.7628, 2.8973, -0.0698, 2.8973, 3.7525, 2.8973]\n",
    "\n",
    "roll_angles = []\n",
    "tracking_errors = []\n",
    "\n",
    "\n",
    "for ITER in range(100):\n",
    "    p.stepSimulation()\n",
    "    cameraPosition, cameraOrientation = robot.get_ee_position()\n",
    "    rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "    pil_img = Image.fromarray(rgb)\n",
    "\n",
    "    # Generate candidate bounding boxes (for demonstration, assume we have a function)\n",
    "    candidate_boxes = generate_candidate_boxes(rgb)\n",
    "\n",
    "    best_similarity = -1\n",
    "    best_box = None\n",
    "\n",
    "    # Preprocess all candidate boxes first\n",
    "    crops = []\n",
    "    for box in candidate_boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        cropped_img = pil_img.crop((x1, y1, x2, y2))\n",
    "        crop_tensor = preprocess(cropped_img).unsqueeze(0)  # shape: (1, C, H, W)\n",
    "        crops.append(crop_tensor)\n",
    "\n",
    "    # Stack all crops into a single batch\n",
    "    batch = torch.cat(crops, dim=0).to(device)  # shape: (N, C, H, W) where N = number of candidate boxes\n",
    "\n",
    "    # Run model once on entire batch\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(batch)  # shape: (N, D)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute similarities for all boxes at once\n",
    "    similarities = (image_features @ text_features.T).squeeze()  # shape: (N,)\n",
    "\n",
    "    # Find best matching box\n",
    "    best_idx = similarities.argmax().item()\n",
    "    best_box = candidate_boxes[best_idx]\n",
    "    best_similarity = similarities[best_idx].item()\n",
    "\n",
    "    # Now best_box corresponds to the region where CLIP thinks the described object is\n",
    "    # Extract the center of this box as your target pixel location\n",
    "    (x1, y1, x2, y2) = best_box\n",
    "    u_px = (x1 + x2) // 2\n",
    "    v_px = (y1 + y2) // 2\n",
    "\n",
    "    # Now feed (u_px, v_px) into your visual servoing steps:\n",
    "    imageJacobian = getImageJacobian(u_px, v_px, depth, camera_focal_depth, camera_width, camera_height)\n",
    "    delta_X, delta_Omega = findCameraControl(object_location_desired, [u_px, v_px], imageJacobian)\n",
    "    delta_q = findJointControl(robot, delta_X, delta_Omega)\n",
    "    \n",
    "    current_q = robot.get_current_joint_angles()\n",
    "    new_jointPositions = current_q + delta_q\n",
    "    new_jointPositions = np.clip(new_jointPositions, joint_limits_lower, joint_limits_upper)\n",
    "    robot.set_joint_position(new_jointPositions)\n",
    "    \n",
    "    \n",
    "    tracking_errors.append(np.linalg.norm(object_location_desired - [u_px,v_px]))\n",
    "\n",
    "    # Show debug info\n",
    "    debug_img = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "    cv2.circle(debug_img, (u_px, v_px), 5, (0,0,0), 2)\n",
    "    cv2.circle(debug_img, (int(object_location_desired[0]), int(object_location_desired[1])), 5, (255,0,0), 2)\n",
    "    cv2.rectangle(debug_img, (x1, y1), (x2, y2), (0, 0, 0), 2)  # Red rectangle\n",
    "    \n",
    "\n",
    "    cv2.imshow(\"Camera RGB\", debug_img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "p.disconnect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
